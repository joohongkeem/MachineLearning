# -*- coding: utf-8 -*-
#  Run module(Hot key : F5)
print("-------------------------------------------------")
print("# 20180712","Made by joohongkeem#".rjust(38),sep=' ',end='\n')
print("-------------------------------------------------")


# 공부 시간과 시험 합격 여부에 대한 로지스틱 회귀 분석 모델

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import r2_score


# 시그모이드 함수 선언
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 입력 데이터 선언
X_train = np.array([[10], [20], [100], [1000], [5000], [6000], [7000], [8000], [9000], [10000], [10010], [10020], [14000]])
y_train = np.array([   0,    0,     0,      0,      0,      0,       0,      0,      0,      1,       1,       1,       1])


Logi_model = LogisticRegression(C=1e5)           # 1. 로지스틱 회귀 분석 모델 생성
Logi_model.fit(X_train, y_train)                 # 2. 학습
Logi_predict = Logi_model.predict(X_train)       # 3. 예측

# C : 정규화를 위한 인자 값..
print("Logi_model.coef_ :" , Logi_model.coef_)                   # Logi_model.coef_ : [[0.0020924]]
print("Logi_model.intercept_ :" , Logi_model.intercept_)        # Logi_model.intercept_ : [-19.18871729]



# 선을 그리기위한 데이터
xx = np.linspace(0, 15000, 100)
            # y = linspace(x1,x2)는 x1과 x2 사이에서 균일한 간격의 점 100개로 구성된 행 벡터를 반환합니다.
            # y = linspace(x1,x2,n)은 n개의 점을 생성합니다. 점 사이의 간격은 (x2-x1)/(n-1)입니다.
'''
[    0.           151.51515152   303.03030303   454.54545455
   606.06060606   757.57575758   909.09090909  1060.60606061
  1212.12121212  1363.63636364  1515.15151515  1666.66666667
  1818.18181818  1969.6969697   2121.21212121  2272.72727273
  2424.24242424  2575.75757576  2727.27272727  2878.78787879
  3030.3030303   3181.81818182  3333.33333333  3484.84848485
  3636.36363636  3787.87878788  3939.39393939  4090.90909091
  4242.42424242  4393.93939394  4545.45454545  4696.96969697
  4848.48484848  5000.          5151.51515152  5303.03030303
  5454.54545455  5606.06060606  5757.57575758  5909.09090909
  6060.60606061  6212.12121212  6363.63636364  6515.15151515
  6666.66666667  6818.18181818  6969.6969697   7121.21212121
  7272.72727273  7424.24242424  7575.75757576  7727.27272727
  7878.78787879  8030.3030303   8181.81818182  8333.33333333
  8484.84848485  8636.36363636  8787.87878788  8939.39393939
  9090.90909091  9242.42424242  9393.93939394  9545.45454545
  9696.96969697  9848.48484848 10000.         10151.51515152
 10303.03030303 10454.54545455 10606.06060606 10757.57575758
 10909.09090909 11060.60606061 11212.12121212 11363.63636364
 11515.15151515 11666.66666667 11818.18181818 11969.6969697
 12121.21212121 12272.72727273 12424.24242424 12575.75757576
 12727.27272727 12878.78787879 13030.3030303  13181.81818182
 13333.33333333 13484.84848485 13636.36363636 13787.87878788
 13939.39393939 14090.90909091 14242.42424242 14393.93939394
 14545.45454545 14696.96969697 14848.48484848 15000. 
 '''
sigm = sigmoid(Logi_model.coef_[0][0]*xx + Logi_model.intercept_[0])
'''
[4.63923062e-09 6.36987052e-09 8.74611627e-09 1.20088076e-08
 1.64886283e-08 2.26396219e-08 3.10852101e-08 4.26813793e-08
 5.86034362e-08 8.04651295e-08 1.10482208e-07 1.51696992e-07
 2.08286723e-07 2.85986937e-07 3.92672776e-07 5.39157154e-07
 7.40286667e-07 1.01644633e-06 1.39562560e-06 1.91625519e-06
 2.63110196e-06 3.61261677e-06 4.96027725e-06 6.81066926e-06
 9.35132886e-06 1.28397467e-05 1.76294604e-05 2.42058762e-05
 3.32354473e-05 4.56331855e-05 6.26553321e-05 8.60265543e-05
 1.18114505e-04 1.62169337e-04 2.22652262e-04 3.05686095e-04
 4.19672783e-04 5.76139248e-04 7.90894896e-04 1.08561366e-03
 1.48999255e-03 2.04468971e-03 2.80531038e-03 3.84778952e-03
 5.27561400e-03 7.22942549e-03 9.89962531e-03 1.35426141e-02
 1.85011453e-02 2.52287749e-02 3.43172575e-02 4.65235368e-02
 6.27891626e-02 8.42391728e-02 1.12140141e-01 1.47790839e-01
 1.92320300e-01 2.46387511e-01 3.09824021e-01 3.81328898e-01
 4.58376321e-01 5.37467053e-01 6.14716149e-01 6.86586945e-01
 7.50492651e-01 8.05067393e-01 8.50089438e-01 8.86183174e-01
 9.14461118e-01 9.36219026e-01 9.52728623e-01 9.65123935e-01
 9.74356480e-01 9.81192574e-01 9.86232041e-01 9.89935029e-01
 9.92649496e-01 9.94635853e-01 9.96087545e-01 9.97147493e-01
 9.97920884e-01 9.98484906e-01 9.98896090e-01 9.99195772e-01
 9.99414146e-01 9.99573250e-01 9.99689158e-01 9.99773592e-01
 9.99835095e-01 9.99879893e-01 9.99912522e-01 9.99936288e-01
 9.99953597e-01 9.99966204e-01 9.99975386e-01 9.99982073e-01
 9.99986944e-01 9.99990491e-01 9.99993074e-01 9.99994956e-01]
 '''
Logi_precision, recall, thresholds = roc_curve(y_train, Logi_model.predict(X_train))
# Logi_Precision : FPR(False Positive Rate), '1-특이도' = 0인 케이스에 대해 1로 잘못 예측한 비율
#                 -> 스팸 메일이 아닌 것을 스팸 메일로 판단함
#                 (참고) 특이도 : 0인 케이스에 대해 0이라고 예측
#
# recall : TPR(True Positive Rate), '민감도' = 1인 케이스에 대해 1이라고 예측한 비율
#                 -> 스팸 메일을 스팸 메일이라 판단함
print('threshold :',thresholds)

# 새로운 검증 데이터
X_test =np.array([[8500], [12000]])
y_test = Logi_model.predict(X_test)

# 선형 모델
#
Linear_model = LinearRegression().fit(X_train, y_train)     # 1. 모델 생성 + 2. 학습
Linear_predict = Linear_model.predict(X_train)              # 3. 예측


# 평가
#

print("Logistic r2_score :" , r2_score(y_train, Logi_predict))
print("Linear   r2_score :" , r2_score(y_train, Linear_predict))

print(y_test)

plt.plot(xx, sigm, label = 'sigmoid', c='blue')
plt.plot(X_train, Linear_predict, label ='linear Regression',  linestyle='-', c='gray' )
plt.scatter(X_train, y_train, marker='o', label='Training Data', s=100)
plt.scatter(X_train, Logi_predict, marker='x', label='Predict Data', c='red', s=100, lw=2, alpha=0.5)
plt.scatter(X_test, y_test, marker='^', label='new_predict', c='green', s=100)

plt.xlim(0, 15000)
plt.legend()
plt.show()

plt.ylim(0, 1.1)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.plot(Logi_precision, recall)

plt.show()
